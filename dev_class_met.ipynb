{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for meteorological data handling\n",
    "Developing processes of classes aiming to create ther class files to be invoked.<br>\n",
    "（財）気象業務支援センター「気象データベース」の図化解析用に開発しているが，pandas DataFrameを活用することで，Classの汎用化を目指す．<br>\n",
    "現在のところ，「地上観測」のみに対応している．「アメダス」は今後対応する予定である．<br>\n",
    "1991年以降は毎時データで全天日射量あり（一部観測所のみ），1990年以前は3時間データで全天日射量，日照時間，降水量がない\n",
    "#### Author: Jun Sasaki, Coded on Sep. 9, 2017, Revised on January 11, 2019\n",
    "## 課題： 1990年以前の3時間データと全天日射量対応，アメダスデータ対応\n",
    "1990年以前と1991年以降を同時に読み込むことは可能だが，時間間隔が異なる．任意の時間間隔にリサンプリングできるようにする．<br>\n",
    "全天日射や欠損値への対応を検討する．<br>\n",
    "RMKが2の場合は0の値が入っているようであるが，このままでよいか要検討．全天日射では2は夜間に相当するので，0とするのでよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import subprocess\n",
    "from pandas.tseries.offsets import Hour\n",
    "from dateutil.parser import parse\n",
    "#import json  # json cannot manipulate datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib.dates import date2num, YearLocator, MonthLocator, DayLocator, DateFormatter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Met:\n",
    "    '''気象データベース・地上観測DVD，アメダスDVDの時系列データ抽出\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |リマーク|            解                                  説          |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ０   |観測値が未作成の場合                                        |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   １   |欠測                                                        |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ２   |観測していない場合                                          |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ３   |日の極値が真の値以下の場合，該当現象がない推定値の場合      |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ４   |日の極値が真の値以上の場合，該当現象がない地域気象観測データ|\n",
    "  |        |を使用する場合                                              |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ５   |推定値が含まれる場合，または２４回平均値で欠測を含む場合    |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  |   ６   |該当する現象がない場合（降水量，日照時間，降雪，積雪，最低海|\n",
    "  |        |面気圧）                                                    |\n",
    "  +--------+------------------------------------------------------------+\n",
    "　|   ７ 　|日の極値の起時が前日の場合　　　　　　　　　　　　　　　　　|\n",
    "　+--------+------------------------------------------------------------+\n",
    "　|   ８ 　|正常な観測値　　　　　　　　　　　　　　　　　　　　　　　　|\n",
    "　+--------+------------------------------------------------------------+\n",
    "  |   ９   |日の極値の起時が翌日の場合，または１９９０年までの８０型地上|\n",
    "  |        |気象観測装置からの自動取得値                                |\n",
    "  +--------+------------------------------------------------------------+\n",
    "  '''\n",
    "    rmk = {\"0\":\"観測値が未作成の場合\", \"1\": \"欠測\", \"2\":\"観測していない場合\", \\\n",
    "           \"3\":\"日の極値が真の値以下の場合，該当現象がない推定値の場合\", \\\n",
    "           \"4\":\"日の極値が真の値以上の場合，該当現象がない地域気象観測データを使用する場合\", \\\n",
    "           \"5\":\"推定値が含まれる場合，または２４回平均値で欠測を含む場合\", \\\n",
    "           \"6\":\"該当する現象がない場合（降水量，日照時間，降雪，積雪，最低海面気圧）\", \\\n",
    "           \"7\":\"日の極値の起時が前日の場合\", \"8\":\"正常な観測値\", \\\n",
    "           \"9\":\"日の極値の起時が翌日の場合，または１９９０年までの８０型地上気象観測装置からの自動取得値\"}\n",
    "    nan_0_1_RMK = [0, 1, 2]\n",
    "    def __init__(self, datetime_ini, datetime_end, stn, dir):\n",
    "        self.datetime_ini = parse(datetime_ini)\n",
    "        self.datetime_end = parse(datetime_end)\n",
    "        print(\"Initial datetime = \", self.datetime_ini)\n",
    "        print(\"End datetime = \", self.datetime_end)\n",
    "        self.stn = stn\n",
    "        self.dir = dir\n",
    "\n",
    "    def set_missing_values(self, df, rmk_cols, rmk_nans):\n",
    "        '''DataFrame dfを入力し，RMK列が欠損値条件を満たす行の一つ前の列の値をnp.nanに置換する'''\n",
    "        ### rmk_cols = [col for col in df.columns if 'RMK' in col]  ### RMK列名のリスト\n",
    "        for rmk_col in rmk_cols:\n",
    "            for rmk_nan in rmk_nans:\n",
    "                idx = df.columns.get_loc(rmk_col) - 1  ### RMKに対応する値の列インデックス\n",
    "                df.iloc[:, idx].mask(df[rmk_col] == rmk_nan, np.nan, inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Met_GWO 気象データベース・地上観測 時別値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Met_GWO(Met):\n",
    "    '''気象データベース・地上観測DVDの時別値（1990年以前は3時間間隔）データ抽出\n",
    "       Directory name is suppoed to be stn + \"/\", file name supposed to be stn + year + \".csv\"'''\n",
    "    col_names_jp = [\"観測所ID\",\"観測所名\",\"ID1\",\"年\",\"月\",\"日\",\"時\", \\\n",
    "                    \"現地気圧(0.1hPa)\",\"ID2\",\"海面気圧(0.1hPa)\",\"ID3\", \\\n",
    "                    \"気温(0.1degC)\",\"ID4\",\"蒸気圧(0.1hPa)\",\"ID5\", \\\n",
    "                    \"相対湿度(%)\",\"ID6\",\"風向(1(NNE)～16(N))\",\"ID7\",\"風速(0.1m/s)\",\"ID8\", \\\n",
    "                    \"雲量(10分比)\",\"ID9\",\"現在天気\",\"ID10\",\"露天温度(0.1degC)\",\"ID11\", \\\n",
    "                    \"日照時間(0.1時間)\",\"ID12\",\"全天日射量(0.01MJ/m2/h)\",\"ID13\", \\\n",
    "                    \"降水量(0.1mm/h)\",\"ID14\"]\n",
    "    col_names = [\"KanID\",\"Kname\",\"KanID_1\",\"YYYY\",\"MM\",\"DD\",\"HH\",\"lhpa\",\"lhpaRMK\", \\\n",
    "                 \"shpa\",\"shpaRMK\",\"kion\",\"kionRMK\",\"stem\",\"stemRMK\",\"rhum\",\"rhumRMK\", \\\n",
    "                 \"muki\",\"mukiRMK\",\"sped\",\"spedRMK\",\"clod\",\"clodRMK\",\"tnki\",\"tnkiRMK\", \\\n",
    "                 \"humd\",\"humdRMK\",\"lght\",\"lghtRMK\",\"slht\",\"slhtRMK\",\"kous\",\"kousRMK\"]\n",
    "    col_items_jp = [\"現地気圧(hPa)\",\"海面気圧(hPa)\", \\\n",
    "                    \"気温(degC)\",\"蒸気圧(hPa)\", \\\n",
    "                    \"相対湿度(0-1)\",\"風向(0-360)\",\"風速(m/s)\", \\\n",
    "                    \"雲量(0-1)\",\"現在天気\",\"露天温度(degC)\", \\\n",
    "                    \"日照時間(時間)\",\"全天日射量(W/m2)\", \\\n",
    "                    \"降水量(mm/h)\",\"風速u(m/s)\",\"風速v(m/s)\"]\n",
    "    col_items =  [\"lhpa\", \\\n",
    "                  \"shpa\",\"kion\",\"stem\",\"rhum\", \\\n",
    "                  \"muki\",\"sped\",\"clod\",\"tnki\", \\\n",
    "                  \"humd\",\"lght\",\"slht\",\"kous\",\"u\",\"v\"]\n",
    "\n",
    "    def __init__(self, datetime_ini = \"2014-1-10 15:00:00\", datetime_end = \"2014-6-1 00:00:00\", \\\n",
    "                 stn = \"Tokyo\", dir = \"../../../met/GWO/\"):\n",
    "        super().__init__(datetime_ini, datetime_end, stn, dir)\n",
    "\n",
    "        self.names_jp = Met_GWO.col_names_jp\n",
    "        self.names = Met_GWO.col_names\n",
    "        self.items_jp = Met_GWO.col_items_jp\n",
    "        self.items = Met_GWO.col_items\n",
    "\n",
    "        ### the values of rmk to be set as NaN (RMK=0, 1, 2)\n",
    "        self.rmk_nan01 = [\"0\", \"1\"]  ### sghtとslhtの夜間はRMK=2なので，RMK=2を欠損値としない\n",
    "        self.rmk_nan = [\"0\", \"1\", \"2\"]  ### clodとtnkiは3時間間隔で，観測なしのRMK=2を欠損値とする必要がある\n",
    "        self.__df, self.__df_org, self.__df_interp, self.__df_interp_1H = self.create_df()\n",
    "\n",
    "    ### propertyの名称をcreate_dfでの名称から変更している．通常使うself.__df_interp_1Hをself.dfと簡単に呼べる様にするため\n",
    "    @property\n",
    "    def df(self):\n",
    "        '''1時間間隔で欠損値を可能な限り補間したDataFrame'''\n",
    "        return self.__df_interp_1H\n",
    "    @property\n",
    "    def df_org(self):\n",
    "        '''元々の時間間隔で欠損値を可能な限り補間したDataFrame．1990年を境に時間間隔が異なる'''\n",
    "        return self.__df_interp\n",
    "    @property\n",
    "    def df_na_rmk(self):\n",
    "        '''元々の時間間隔で欠損値を含むDataFrame．df_naと同様だが，RMK値を欠損値にする'''\n",
    "        return self.__df\n",
    "    @property\n",
    "    def df_na(self):\n",
    "        '''元々の時間間隔で欠損値を含むDataFrame．df_na_rmkと同様だが変数値を欠損値にする'''\n",
    "        return self.__df_org\n",
    "\n",
    "    def to_csv(self, df, fo_path='./df.csv'):\n",
    "        '''DataFrame dfをCSV出力するmethod\n",
    "           引数 df: DataFrame（必須）, fo_path=出力先ファイルのpath'''\n",
    "        df.to_csv(fo_path, encoding='utf-8', )  ### CSVで出力する．デフォルトではWindows版ではShiftJISとなるため，UTF-8を明示する．\n",
    "        cmd = 'nkf -w -Lu --overwrite ' + fo_path  ### 改行コードをLinuxタイプのLFに変更しておく（Linuxとの互換性のため）\n",
    "        subprocess.call(cmd)\n",
    "    def read_csv(self, fi_path):\n",
    "        '''fi_pathからCSV fileを読み込みDataFrameを作って返す\n",
    "           引数 fi_path=CSV fileのpath  戻り値 DataFrame\n",
    "        '''\n",
    "        return pd.read_csv(fi_path, index_col=0, parse_dates=True)  ### 出力したCSVをDataFrameとして読み込む\n",
    "\n",
    "    '''以下は隠避されたmethod．意味が分からなくても使うには困らない'''\n",
    "    def create_df(self, interp=True):\n",
    "        '''interp: True=欠損値補間を実行'''\n",
    "        start = self.datetime_ini\n",
    "        end   = self.datetime_end\n",
    "        if start >= end:\n",
    "            print(\"ERROR: start >= end\")\n",
    "            sys.exit()\n",
    "        Ys, Ye = int(start.strftime(\"%Y\")), int(end.strftime(\"%Y\"))\n",
    "        fdir = self.dir + self.stn + \"/\"  # data dir\n",
    "        print(\"Data directory = \", fdir)\n",
    "        fstart = glob.glob(fdir + self.stn + str(Ys-1) + \".csv\") # check Ys-1 exists?\n",
    "        if len(fstart) == 1:\n",
    "            Ys += -1\n",
    "        else:\n",
    "            fstart = glob.glob(fdir + self.stn + str(Ys) + \".csv\")\n",
    "        if len(fstart) != 1:\n",
    "            print(fstart)\n",
    "            print('ERROR: fstart does not exist or has more than one file.')\n",
    "            sys.exit()\n",
    "        fend = glob.glob(fdir + self.stn + str(Ye+1) + \".csv\")  # Ye+1 exists?\n",
    "        if len(fend) == 1:\n",
    "            Ye += 1\n",
    "        else:\n",
    "            fend = glob.glob(fdir + self.stn + str(Ye) + \".csv\")\n",
    "        if len(fend) != 1:\n",
    "            print('fend= ', fend)\n",
    "            print('ERROR: fend does not exist or has more than one file.')\n",
    "            sys.exit()\n",
    "\n",
    "        tsa = []  ### RMKの欠損値を考慮\n",
    "        tsa_org = []  ### RMKの欠損値を考慮しない，オリジナルと同一\n",
    "        fyears = list(range(Ys, Ye+1)) # from Ys to Ye\n",
    "\n",
    "        ### Reading csv files\n",
    "        ### カラム毎に欠損値を指定する\n",
    "        ### 欠損値を考慮しないデータフレームも併せて作成する\n",
    "        na_values =  {\"lhpaRMK\":self.rmk_nan, \\\n",
    "                      \"shpaRMK\":self.rmk_nan,\"kionRMK\":self.rmk_nan,\"stemRMK\":self.rmk_nan,\"rhumRMK\":self.rmk_nan, \\\n",
    "                      \"mukiRMK\":self.rmk_nan,\"spedRMK\":self.rmk_nan,\"clodRMK\":self.rmk_nan,\"tnkiRMK\":self.rmk_nan, \\\n",
    "                      \"humdRMK\":self.rmk_nan,\"lghtRMK\":self.rmk_nan01,\"slhtRMK\":self.rmk_nan01,\"kousRMK\":self.rmk_nan}\n",
    "        for year in fyears:\n",
    "            file = fdir + self.stn + str(year) + \".csv\"\n",
    "            print(file)\n",
    "            tsa.append(pd.read_csv(file, header = None, names = self.names, \\\n",
    "                 parse_dates=[[3,4,5]], na_values = na_values))\n",
    "            tsa_org.append(pd.read_csv(file, header = None, names = self.names, \\\n",
    "                 parse_dates=[[3,4,5]]))\n",
    "            \n",
    "        def merge_df(tsa):\n",
    "            '''Create df from tsa'''\n",
    "            df = pd.concat(tsa)\n",
    "            df.index = [x + y * Hour() for x,y in zip(df['YYYY_MM_DD'],df['HH'])]\n",
    "            df.drop(\"YYYY_MM_DD\", axis=1, inplace=True)\n",
    "            df.drop(\"HH\", axis=1, inplace=True)\n",
    "            df=df[start:end]\n",
    "            return df\n",
    "        df = merge_df(tsa)  ### 欠損値を考慮したDataFrame\n",
    "        df_org = merge_df(tsa_org)  ### 欠損値を無視した，元データと同じDataFrame\n",
    "\n",
    "        ### Check missing values\n",
    "        check_list=[\"lhpa\",\"shpa\",\"kion\",\"stem\",\"rhum\",\"muki\",\"sped\",\"clod\",\"tnki\", \\\n",
    "                    \"humd\",\"lght\",\"slht\",\"kous\"]\n",
    "        for lst in check_list:\n",
    "            rmk = lst + \"RMK\"\n",
    "            mask = df[rmk] == 1 # 1:missing value\n",
    "            missing = df[lst][mask]\n",
    "        if len(missing)==0:\n",
    "            #print(\"No missing values in\", lst)\n",
    "            pass\n",
    "        else:\n",
    "            print(missing, \"in\", lst)\n",
    "\n",
    "        ### Unit conversions\n",
    "        '''\n",
    "        TEEM出力：海面気圧(hPa), 気温(degC)，蒸気圧(hPa)，相対湿度(0-1)，風速(m/s)，\n",
    "                  風向(deg), 雲量(0-1), 全天日射量(W/m2), 降水量(m/h)\n",
    "        '''\n",
    "        def unit_conversion(df):\n",
    "            df['lhpa']=df['lhpa']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "            df['shpa']=df['shpa']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "            df['kion']=df['kion']/1.0e1 # [0.1degC] -> [degC]\n",
    "            df['stem']=df['stem']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "            df['rhum']=df['rhum']/1.0e2 # [%] -> [0-1]\n",
    "            df['muki']=-90.0 - df['muki'] * 22.5 # [0-16] -> [deg]  0=NA, 1=NNE, .., 8=S, ..., 16=N\n",
    "            df['muki']=df['muki'] % 360.0 # 0-360\n",
    "            df['sped']=df['sped']/1.0e1 # [0.1m/s] -> [m/s]\n",
    "            df['clod']=df['clod']/1.0e1 # [0-10] -> [0-1]\n",
    "            df['humd']=df['humd']/1.0e1 # [0.1degC] -> [degC]\n",
    "            df['lght']=df['lght']/1.0e1 # [0.1h] -> [h]\n",
    "            df['slht']=df['slht']*1.0e4/3.6e3 # [0.01MJ/m2/h] -> [J/m2/s] = [W/m2]\n",
    "            ### wind vector (u,v)\n",
    "            rad = df[\"muki\"].values * 2 * np.pi / 360.0\n",
    "            (u, v) = df[\"sped\"].values * (np.cos(rad), np.sin(rad))\n",
    "            df[\"u\"] = u\n",
    "            df[\"v\"] = v\n",
    "        unit_conversion(df)\n",
    "        unit_conversion(df_org)\n",
    "\n",
    "        \n",
    "        def df_interp(df, df_org):\n",
    "            '''RMKをチェックして欠損値を見つけ，RMKは元の整数を保持したまま，値を欠損値にし，\n",
    "               それを適切に補間したDataFrame df_interpを作る．\n",
    "               さらに，df_interpをすべて1時間間隔にreindexし，欠損値を埋めたdf_interp_1Hを作る\n",
    "            '''\n",
    "            ### RMKがNaNである行rowsを抽出する Find rows containing NaN\n",
    "            rows = pd.isnull(df).any(axis=1).nonzero()[0]\n",
    "            ### RMKがNaNである列colsを見つける．次行のコメントはrow=280において，列を見つける方法を例として示す\n",
    "            ### cols = pd.isnull(df0.iloc[280]).nonzero()[0]  ### in case of row =280\n",
    "            ### すべてのrowsについてRMKがNaNである列を見つけ，該当する行と列の番号を持つリストnan_ar[rows][cols]を作る\n",
    "            ### nan_ar[0][0]は一つ目の行（0）の行番号，nan_ar[0][1]はその行の列番号を含むarrayを返す．\n",
    "            ###   一般に一つの行には複数のNaNが含まれるので，列番号はarrayになる\n",
    "            ### すべての行についてリスト内包を用い，リストnan_arを作る．ただし，列番号-1とすることで，RMKのデータ値の列番号とする．\n",
    "            nan_ar=[[row, pd.isnull(df.iloc[row]).nonzero()[0]-1] for row in rows]\n",
    "            ### print(nan_ar)\n",
    "            na_rows=[nan_ar[row][0] for row in range(len(rows))]  ### rows containing NaN\n",
    "            ### print(na_rows)\n",
    "            df_interp=df_org\n",
    "            for row in range(len(nan_ar)):\n",
    "                df_interp.iloc[nan_ar[row][0],nan_ar[row][1]]=np.nan\n",
    "            ### 欠損値を内挿したdf_interpを作る\n",
    "            df_interp.interpolate(method='time', inplace=True)\n",
    "            ### 1990年以前の3時間間隔を1時間間隔にする\n",
    "            ### 1時間間隔のインデックスを作る\n",
    "            new_index = pd.date_range(self.datetime_ini, self.datetime_end, freq='1H')\n",
    "            ### ffillを適用するカラム名のリスト\n",
    "            cols_ffill=['KanID', 'Kname', 'KanID_1', 'lhpaRMK', 'shpaRMK', 'kionRMK', 'stemRMK', 'rhumRMK', 'mukiRMK', 'spedRMK', \\\n",
    "                        'clodRMK', 'tnkiRMK', 'humdRMK', 'lghtRMK', 'slhtRMK', 'kousRMK']\n",
    "            ### カラム全体のリストからffillを適用するカラムを削除するラムダ関数dellistを定義\n",
    "            dellist = lambda items, sublist: [item for item in items if item not in sublist]\n",
    "            ### 内挿を適用するカラム名のリストをつくる\n",
    "            cols_interp=dellist(df_interp.columns, cols_ffill)\n",
    "            ### print(cols_interp)\n",
    "            ### 1時間間隔のインデックスを適用し，ffillを適用すべきカラムを対象に補完実行．結果をdf_ffillに入れる\n",
    "            df_ffill = df_interp.reindex(new_index).loc[:, cols_ffill].fillna(method='ffill')\n",
    "            ### カラムKname以外のカラムのdtypeをintに戻す（NaNを扱うとintだったものがfloatに変更されてしまう）\n",
    "            df_ffill[dellist(df_ffill.columns, ['Kname'])] = df_ffill[dellist(df_ffill.columns, ['Kname'])].astype(int)\n",
    "            ### 1時間間隔のインデックスを適用し，時間内挿すべきカラムを対象に内挿実行．結果をdf_interpに入れる\n",
    "            df_interp_1H = df_interp.reindex(new_index).loc[:, cols_interp].interpolate(method='time')\n",
    "            ### df_ffillとdf_interpを連結し，カラムの順序をdf1と同じとしたデータフレームdfを作る\n",
    "            ### これで一応完成だが，1990年以前の全天日射量，日照時間，降水量への対応を今後進める\n",
    "            df_interp_1H = pd.concat([df_ffill, df_interp_1H], axis=1)[df_interp.columns]\n",
    "            return df_interp, df_interp_1H\n",
    "        \n",
    "        if interp:\n",
    "            df_interp, df_interp_1H = df_interp(df, df_org)\n",
    "            return df, df_org, df_interp, df_interp_1H\n",
    "        else:\n",
    "            print('Interpolation is deactivated.')\n",
    "            return df_org\n",
    "\n",
    "    #@staticmethod\n",
    "    #def check_data():\n",
    "    #    '''CSV気象データをチェックするための静的メソッド'''\n",
    "    #    print(self.names_jp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データチェック用のClass\n",
    "千葉の2010年，2011年に欠損行があることが判明したので，CSVデータチェック用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Met_GWO_check(Met_GWO):\n",
    "    def __init__(self, datetime_ini = \"2014-1-1 15:00:00\", datetime_end = \"2014-6-1 00:00:00\", \\\n",
    "                 stn = \"Tokyo\", dir = \"../GWO/Hourly/\"):\n",
    "        Met.__init__(self, datetime_ini, datetime_end, stn, dir)  ### Class Met_GWO inherits Class Met.\n",
    "        self.names_jp = Met_GWO.col_names_jp\n",
    "        self.names = Met_GWO.col_names\n",
    "        self.items_jp = Met_GWO.col_items_jp\n",
    "        self.items = Met_GWO.col_items\n",
    "\n",
    "        ### the values of rmk to be set as NaN (RMK=0, 1, 2)\n",
    "        self.rmk_nan01 = [\"0\", \"1\"]  ### sghtとslhtの夜間はRMK=2なので，RMK=2を欠損値としない\n",
    "        self.rmk_nan = [\"0\", \"1\", \"2\"]  ### clodとtnkiは3時間間隔で，観測なしのRMK=2を欠損値とする必要がある\n",
    "        self.__df_org = super().create_df(interp=False)  ### ここは必ずFalseにする\n",
    "    @property\n",
    "    def df_org(self):\n",
    "        return self.__df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_ini = \"2012-1-1 00:00:00\"\n",
    "datetime_end = \"2012-12-31 23:00:00\"\n",
    "stn = \"Chiba\"\n",
    "dir =  \"../GWO/Hourly/\"\n",
    "met_check = Met_GWO_check(datetime_ini=datetime_ini, datetime_end=datetime_end, stn=stn, dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_index = pd.date_range(datetime_ini, datetime_end, freq='H')\n",
    "list(filter(lambda x: not x in met_check.df_org.index, datetime_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_check.df_org.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"2011-01-01 00:00:00\" in met_check.df_org.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気象データベース・地上観測・日別値用Class GWO Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Met_GWO_daily(Met):\n",
    "    '''気象データベース・地上観測DVDの日別値データ抽出\n",
    "       全天日射量日別値の単位について：1961-1980は1cal/cm2，1981以降は0.1MJ/m2\n",
    "       Directory name is suppoed to be stn + \"/\", file name supposed to be stn + year + \".csv\"'''\n",
    "    def __init__(self, datetime_ini = \"2014-1-10 15:00:00\", datetime_end = \"2014-6-1 00:00:00\", \\\n",
    "                 stn = \"Tokyo\", dir = \"../../../met/GWO/Daily/\"):\n",
    "        super().__init__(datetime_ini, datetime_end, stn, dir)\n",
    "        #super().__init__()\n",
    "        self.names_jp = [\"観測所ID\",\"観測所名\",\"ID1\",\"年\",\"月\",\"日\", \\\n",
    "                         \"平均現地気圧(0.1hPa)\",\"ID2\",\"平均海面気圧(0.1hPa)\",\"ID3\", \"最低海面気圧\", \\\n",
    "                         \"ID4\", \"平均気温(0.1degC)\",\"ID5\", \"最高気温(0.1degC)\",\"ID6\", \"最低気温(0.1degC)\",\"ID7\", \\\n",
    "                         \"平均蒸気圧(0.1hPa)\",\"ID8\", \"平均相対湿度(%)\", \"ID9\", \"最小相対湿度(%)\", \"ID10\", \\\n",
    "                         \"平均風速(0.1m/s)\",\"ID11\", \"最大風速(0.1m/s)\",\"ID12\", \"最大風速風向(1(NNE)～16(N))\",\"ID13\", \\\n",
    "                         \"最大瞬間風速(0.1m/s)\",\"ID14\", \"最大瞬間風向(1(NNE)～16(N))\", \"ID15\", \\\n",
    "                         \"平均雲量(0.1)\",\"ID16\", \"日照時間(0.1時間)\", \"ID17\", \"全天日射量(0.1MJ/m2)\", \"ID18\", \\\n",
    "                         \"蒸発量(0.1mm)\", \"ID19\", \"日降水量(0.1mm)\", \"ID20\" , \"最大1時間降水量(0.1mm)\", \"ID21\", \\\n",
    "                         \"最大10分間降水量(0.1mm)\", \"ID22\", \"降雪の深さ日合計(cm)\", \"ID23\",  \"日最深積雪(cm)\",\"ID24\", \\\n",
    "                         \"天気概況符号1\", \"ID25\", \"天気概況符号2\", \"ID26\", \"大気現象コード1\", \"大気現象コード1\", \\\n",
    "                         \"大気現象コード2\", \"大気現象コード3\", \"大気現象コード4\", \"大気現象コード5\", \\\n",
    "                         \"降水強風時間\", \"ID27\"]\n",
    "\n",
    "        self.names = [\"KanID\",\"Kname\",\"KanID_1\",\"YYYY\",\"MM\",\"DD\", \"avrLhpa\", \"avrLhpaRMK\", \"avrShpa\", \\\n",
    "                      \"avrShpaRMK\", \"minShpa\", \"minShpaRMK\", \"avrKion\", \"avrKionRMK\", \"maxKion\", \"maxKionRMK\", \\\n",
    "                      \"minKion\", \"minKionRMK\", \"avrStem\", \"avrStemRMK\", \"avrRhum\", \"avrRhumRMK\", \"minRhum\", \\\n",
    "                      \"minRhumRMK\", \"avrSped\", \"avrSpedRMK\", \"maxSped\", \"maxSpedRMK\", \"maxMuki\", \"maxMukiRMK\", \\\n",
    "                      \"maxSSpd\", \"maxSSpdRMK\", \"maxSMuk\", \"maxSMukRMK\", \"avrClod\", \"avrClodRMK\", \"daylght\", \\\n",
    "                      \"daylghtRMK\", \"sunlght\", \"sunlghtRMK\", \"amtEva\", \"amtEvaRMK\", \"dayPrec\", \"dayPrecRMK\", \\\n",
    "                      \"maxHPrc\", \"maxHPrcRMK\", \"maxMPrc\", \"maxMPrcRMK\", \"talSnow\", \"talSnowRMK\", \"daySnow\", \\\n",
    "                      \"daySnowRMK\", \"tenki1\", \"tenki1RMK\", \"tenki2\", \"tenki2RMK\", \"apCode1\", \"apCode2\", \"apCode3\", \\\n",
    "                      \"apCode4\", \"apCode5\", \"strgTim\", \"strgTimRMK\"]\n",
    "                      \n",
    "        self.items_jp = [\"現地気圧(hPa)\",\"海面気圧(hPa)\", \\\n",
    "                         \"気温(degC)\",\"蒸気圧(hPa)\", \\\n",
    "                         \"相対湿度(0-1)\",\"風向(0-360)\",\"風速(m/s)\", \\\n",
    "                         \"雲量(0-1)\",\"現在天気\",\"露天温度(degC)\", \\\n",
    "                         \"日照時間(時間)\",\"全天日射量(W/m2)\", \\\n",
    "                         \"降水量(mm/h)\",\"風速u(m/s)\",\"風速v(m/s)\"]\n",
    "\n",
    "        self.items = [\"lhpa\", \\\n",
    "                      \"shpa\",\"kion\",\"stem\",\"rhum\", \\\n",
    "                      \"muki\",\"sped\",\"clod\",\"tnki\", \\\n",
    "                      \"humd\",\"lght\",\"slht\",\"kous\",\"u\",\"v\"]\n",
    "        ### the values of rmk to be set as NaN (RMK=0, 1, 2)\n",
    "        self.rmk_nan01 = [\"0\", \"1\"]  ### sghtとslhtの夜間はRMK=2なので，RMK=2を欠損値としない\n",
    "        self.rmk_nan = [\"0\", \"1\", \"2\"]  ### clodとtnkiは3時間間隔で，観測なしのRMK=2を欠損値とする必要がある\n",
    "        # self.__df, self.__df_org, self.__df_interp, self.__df_interp_1H = self.__create_df()\n",
    "        ### __create_df()でDataFrameを作る\n",
    "        self.__df_org, self.__df = self.__create_df()\n",
    "\n",
    "    ### propertyの名称をcreate_dfでの名称から変更している．通常使うself.__df_interp_1Hをself.dfと簡単に呼べる様にするため\n",
    "    @property\n",
    "    def df_org(self):\n",
    "        '''欠損値未処理のDataFrameにアクセスする'''\n",
    "        return self.__df_org\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        '''欠損値処理したDataFrameにアクセスする'''\n",
    "        return self.__df\n",
    "\n",
    "\n",
    "    def to_csv(self, df, fo_path='./df.csv'):\n",
    "        '''DataFrame dfをCSV出力するmethod\n",
    "           引数 df: DataFrame（必須）, fo_path=出力先ファイルのpath'''\n",
    "        df.to_csv(fo_path, encoding='utf-8', )  ### CSVで出力する．デフォルトではWindows版ではShiftJISとなるため，UTF-8を明示する．\n",
    "        cmd = 'nkf -w -Lu --overwrite ' + fo_path  ### 改行コードをLinuxタイプのLFに変更しておく（Linuxとの互換性のため）\n",
    "        subprocess.call(cmd)\n",
    "\n",
    "    def read_csv(self, fi_path):\n",
    "        '''メソッドto_csvで出力したcsvファイルを読み込みDataFrameを返す\n",
    "           引数 fi_path=CSV fileのpath  戻り値 DataFrame\n",
    "        '''\n",
    "        try:\n",
    "            return pd.read_csv(fi_path, index_col=0, parse_dates=True)  ### 出力したCSVをDataFrameとして読み込む\n",
    "        except:\n",
    "            print('Error in reading csv of ', fi_path)\n",
    "\n",
    "    '''以下は隠避されたmethod．意味が分からなくても使うには困らない'''\n",
    "\n",
    "    def __unit_conversion(self, df):\n",
    "        '''DataFrameを受け取り，カラムの単位を変換する．風速ベクトルを定義する．'''\n",
    "        df['avrLhpa']=df['avrLhpa']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "        df['avrShpa']=df['avrShpa']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "        df['minShpa']=df['minShpa']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "        df['avrKion']=df['avrKion']/1.0e1 # [0.1degC] -> [degC]\n",
    "        df['maxKion']=df['maxKion']/1.0e1 # [0.1degC] -> [degC]\n",
    "        df['minKion']=df['minKion']/1.0e1 # [0.1degC] -> [degC]\n",
    "        df['avrStem']=df['avrStem']/1.0e1 # [0.1hPa] -> [hPa]\n",
    "        df['avrRhum']=df['avrRhum']/1.0e2 # [%] -> [0-1]\n",
    "        df['avrSped']=df['avrSped']/1.0e1 # [0.1m/s] -> [m/s]\n",
    "        df['maxSped']=df['maxSped']/1.0e1 # [0.1m/s] -> [m/s]\n",
    "        df['maxMuki']=-90.0 - df['maxMuki'] * 22.5 # [0-16] -> [deg]  0=NA, 1=NNE, .., 8=S, ..., 16=N\n",
    "        df['maxMuki']=df['maxMuki'] % 360.0 # 0-360\n",
    "        df['maxSSpd']=df['maxSSpd']/1.0e1 # [0.1m/s] -> [m/s]\n",
    "        df['maxSMuk']=-90.0 - df['maxSMuk'] * 22.5 # [0-16] -> [deg]  0=NA, 1=NNE, .., 8=S, ..., 16=N\n",
    "        df['maxSMuk']=df['maxSMuk'] % 360.0 # 0-360\n",
    "        df['avrClod']=df['avrClod']/1.0e1 # [0-10] -> [0-1]\n",
    "        df['daylght']=df['daylght']/1.0e1 # 0.1h -> h\n",
    "        ### 全天日射量日別値の単位について：1961-1980は1cal/cm2，1981以降は0.1MJ/m2\n",
    "        ### 左辺は df.loc[,] が必要\n",
    "        ### df['1981':]['sunlght']=df['1981':]['sunlght']*1.0e5/3.6e3/24.0 ### [0.1MJ/m2/day] -> [J/m2/s] = [W/m2]\n",
    "        ### df[:'1980']['sunlght']=df[:'1980']['sunlght']*4.2*1.0e4/3.6e3/24.0 ### [1calcm2/day] -> [J/m2/s] = [W/m2]\n",
    "        df.loc['1981':, 'sunlght']=df['1981':]['sunlght']*1.0e5/3.6e3/24.0 ### [0.1MJ/m2/day] -> [J/m2/s] = [W/m2]\n",
    "        df.loc[:'1980', 'sunlght']=df[:'1980']['sunlght']*4.2*1.0e4/3.6e3/24.0 ### [1calcm2/day] -> [J/m2/s] = [W/m2]\n",
    "        df['amtEva']=df['amtEva']/1.0e1 # 0.1mm -> 1 mm\n",
    "        df['dayPrec']=df['dayPrec']/1.0e1 # 0.1mm -> 1mm\n",
    "        df['maxHPrc']=df['maxHPrc']/1.0e1 # 0.1mm -> 1mm\n",
    "        df['maxMPrc']=df['maxMPrc']/1.0e1 # 0.1mm -> 1mm\n",
    "        df['talSnow']=df['talSnow'] # cm\n",
    "        df['daySnow']=df['daySnow'] # cm           \n",
    "        ### wind vector (u,v)\n",
    "        rad = df[\"maxMuki\"].values * 2 * np.pi / 360.0\n",
    "        (umax, vmax) = df[\"maxSped\"].values * (np.cos(rad), np.sin(rad))\n",
    "        df[\"umax\"] = umax\n",
    "        df[\"vmax\"] = vmax\n",
    "        (u, v) = df[\"avrSped\"].values * (np.cos(rad), np.sin(rad))\n",
    "        df[\"u\"] = u\n",
    "        df[\"v\"] = v\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def __create_df(self):\n",
    "        start = self.datetime_ini\n",
    "        end   = self.datetime_end\n",
    "        ### Ys, Yeの妥当性と読込みcsvファイルの存在チェック\n",
    "        if start >= end:\n",
    "            print(\"ERROR: start >= end is incorrect.\")\n",
    "            sys.exit()\n",
    "        Ys, Ye = int(start.strftime(\"%Y\")), int(end.strftime(\"%Y\"))\n",
    "        fdir = self.dir + self.stn + \"/\"  # data dir\n",
    "        print(\"Data directory path = \", fdir)\n",
    "        fstart = glob.glob(fdir + self.stn + str(Ys) + \".csv\")\n",
    "        if len(fstart) != 1:\n",
    "            print(fstart)\n",
    "            print('ERROR: fstart file does not exist or has more than one file.')\n",
    "            sys.exit()\n",
    "        fend = glob.glob(fdir + self.stn + str(Ye) + \".csv\")\n",
    "        if len(fend) != 1:\n",
    "            print('fend= ', fend)\n",
    "            print('ERROR: fend file does not exist or has more than one file.')\n",
    "            sys.exit()\n",
    "\n",
    "        tsa_org = []  ### RMKの欠損値を考慮しない，オリジナルと同一\n",
    "        fyears = list(range(Ys, Ye+1)) # from Ys to Ye\n",
    "\n",
    "        for year in fyears:\n",
    "            file = fdir + self.stn + str(year) + \".csv\"\n",
    "            print(\"Reading csv file of \", file)\n",
    "            tsa_org.append(pd.read_csv(file, header = None, names = self.names, parse_dates=[[3,4,5]]))\n",
    "            \n",
    "        def create_df(tsa):\n",
    "            '''Create df from tsa'''\n",
    "            df = pd.concat(tsa)\n",
    "            #df.index = [x + y * Hour() for x,y in zip(df['YYYY_MM_DD'],df['HH'])]\n",
    "            df.index = df['YYYY_MM_DD']\n",
    "            df.drop(\"YYYY_MM_DD\", axis=1, inplace=True)\n",
    "            #df.drop(\"HH\", axis=1, inplace=True)\n",
    "            df=df[start:end]\n",
    "            return df\n",
    "        df_org = create_df(tsa_org)  ### 欠損値を無視した，元データと同じDataFrame\n",
    "\n",
    "        df_org = self.__unit_conversion(df_org)\n",
    "        \n",
    "        ### 欠損値を考慮したDataFrame\n",
    "        df = df_org.copy()  ### df = df_org はcopyではない！\n",
    "        rmk_cols = [col for col in df.columns if 'RMK' in col]  ### RMKカラムのリストを作成\n",
    "        missing_rmk = [0, 1]\n",
    "        df = super().set_missing_values(df, rmk_cols, missing_rmk)\n",
    "\n",
    "        return df_org, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWO 日別値のハンドリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_ini = \"1961-12-1 00:00:00\"\n",
    "datetime_end = \"1962-1-31 00:00:00\"\n",
    "stn = \"Tokyo\"\n",
    "dir =  \"../GWO/Daily/\"\n",
    "met = Met_GWO_daily(datetime_ini=datetime_ini, datetime_end=datetime_end, stn=stn, dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df['avrLhpa'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=met.df.iloc[0:5, [3, 4, 5, 6, 9, 10]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMK_cols = [col for col in df.columns if 'RMK' in col]\n",
    "RMK_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 2].mask(df['avrShpaRMK'] == 8, np.nan, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.get_loc('avrShpaRMK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for rmk_col in rmk_cols:\n",
    "            for rmk_nan in rmk_nans:\n",
    "                idx = df.columns.get_loc(rmk_col) - 1  ### RMKに対応する値の列インデックス\n",
    "                df.iloc[:, idx].mask(df[rmk_col] == rmk_nan, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df['1980':'1981'][['sunlght']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 欠損値とすべきRMK値を持つカラムを調べる．Trueのカラムに欠損値相当が存在\n",
    "met.df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmk_cols = [col for col in met.df.columns if 'RMK' in col]\n",
    "rmk_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df.columns.get_loc(\"avrLhpaRMK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df[met.df['avrLhpaRMK'] == 0].iloc[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.df['sunlght']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気象データベース・地上観測・時別値の解析 GWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create an instance for houly data\n",
    "datetime_ini = \"1990-1-1 00:00:00\"\n",
    "datetime_end = \"1992-1-1 00:00:00\"\n",
    "#stn = \"Tokyo\"\n",
    "stn = \"Chiba\"\n",
    "dir =  \"../GWO/Hourly/\"\n",
    "met = Met_GWO(datetime_ini=datetime_ini, datetime_end=datetime_end, stn=stn, dir=dir)\n",
    "print(met.items_jp)\n",
    "print(met.items)\n",
    "print(met.rmk[\"8\"])\n",
    "print(met.rmk_nan)\n",
    "met.to_csv(met.df, \"df.csv\")\n",
    "new_df = met.read_csv(\"df.csv\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以上でデータフレーム完成．\n",
    "# 以下は上で定義したclassを作成する過程を残したものであり，基本的に不要\n",
    "## ただし，以降にあるPlotting用のClassは参考になるはず\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値を確認する．欠損値はほぼ必ず存在するので，このチェックは極めて重要\n",
    "### df0のRMKがNaNの行とdf_orgの同じ行を抽出する\n",
    "### 1990年以前の全天日射量と日照時間は存在しないので，別途データを作成し，最後に処理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 欠損値を含む行を抽出する Extract only rows including missing values\n",
    "### df0はRMKが欠損値に相当するRMKをNaNにしたDataFrame, df_orgはオリジナルのままのDataFrame\n",
    "### df_naは，df0において，NaNを含む行のみを抽出したDataFrame\n",
    "### df_org_naは，df_naのdf0における行番号に相当するdf_orgの行を抽出\n",
    "### df_naとdf_org_naを見比べれば，NaNとしたRMKとそのRMKの元の値を比較できる\n",
    "df0, df_org = met.df_na_rmk, met.df_na  ### インスタンスmet\n",
    "df_na = df0[df0.isnull().any(axis=1)]\n",
    "df_org_na = df_org[df0.isnull().any(axis=1)]\n",
    "df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### しかし，このままでは見づらいので，NaNの列のみを抽出して表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 欠損値を含む列のみを抽出する．欠損値を含まない列は削除する．\n",
    "### Extract only columns including missing values \n",
    "df_na[df_na.columns[df_na.isnull().any()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### df_naが欠損値を含むdf_na_orgの列のみを抽出する．欠損値を含まない列は削除する．\n",
    "### Extract only columns including missing values \n",
    "df_org_na[df_na.columns[df_na.isnull().any()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMKがNaNのデータ値をNaNとし，RMKは元の整数に戻す\n",
    "### こうすることで，Pandasの機能で欠損値処理ができるようになる．そのために，RMKがNaNである行と列の番号を見つけ，その列番号の一つ前の列のデータ値をNaNにする．\n",
    "### まず，RMKがNaNである行と列を見つけ，その行番号と「列番号-1」からなるリストnan_arを作る\n",
    "注意：RMKの「列番号-1」がそのRMKに対応するデータの列番号である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RMKがNaNである行rowsを抽出する Find rows containing NaN\n",
    "rows = pd.isnull(df0).any(axis=1).nonzero()[0]\n",
    "### RMKがNaNである列colsを見つける．次行のコメントはrow=280において，列を見つける方法を例として示す\n",
    "### cols = pd.isnull(df0.iloc[280]).nonzero()[0]  ### in case of row =280\n",
    "### すべてのrowsについてRMKがNaNである列を見つけ，該当する行と列の番号を持つリストnan_ar[rows][cols]を作る\n",
    "### nan_ar[0][0]は一つ目の行（0）の行番号，nan_ar[0][1]はその行の列番号を含むarrayを返す．\n",
    "###   一般に一つの行には複数のNaNが含まれるので，列番号はarrayになる\n",
    "### すべての行についてリスト内包を用い，リストnan_arを作る．ただし，列番号-1とすることで，RMKのデータ値の列番号とする．\n",
    "nan_ar=[[row, pd.isnull(df0.iloc[row]).nonzero()[0]-1] for row in rows]\n",
    "print(nan_ar)\n",
    "na_rows=[nan_ar[row][0] for row in range(len(rows))]  ### rows containing NaN\n",
    "print(na_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得られたnan_arと，元データのDataFrameであるdf_orgを用い，RMKが欠損値に相当するデータをNaNとするDataFrame dfを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_org  ### 元データを含むdf_orgを残しておくため，コピーを作る\n",
    "print(len(nan_ar), nan_ar[0][0], nan_ar[0][1])\n",
    "for row in range(len(nan_ar)):\n",
    "    df1.iloc[nan_ar[row][0],nan_ar[row][1]]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NaNを含む行と列を抽出し，確認する\n",
    "df1[df1.isnull().any(axis=1)][df1.columns[df1.isnull().any()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df1の欠損値に値を入れる\n",
    "様々な考え方があり得るが，内挿するのがよさそう．内挿の方法にもいろいろある．<br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/missing_data.html <br>\n",
    "内挿がうまくできているかを確認するため，内挿された行とその前後の行を抽出し，確認する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 内挿値を含む行番号とその前後の行番号からなるリストを作る\n",
    "na_rows_bf = [[row-1, row, row+1] for row in na_rows]\n",
    "print(na_rows_bf)\n",
    "### リストを1次元化する\n",
    "na_rows_bf = [flatten for inner in na_rows_bf for flatten in inner]\n",
    "print(na_rows_bf)\n",
    "### 内挿値およびその前後の行のdfを表示する（内挿値を含む列のみ抽出）\n",
    "if na_rows_bf[0] < 0:\n",
    "    print(\"Initial row contains NaN.\")\n",
    "    na_rows_bf = na_rows_bf[1:]  ### Out of boundsを避けるため\n",
    "if na_rows_bf[-1] > len(df1)-1:\n",
    "    print(\"Final row contains NaN.\")\n",
    "    na_rows_bf = na_rows_bf[:-1]  ### Out of boundsを避けるため\n",
    "df1.interpolate(method='time').iloc[na_rows_bf][df1.columns[df1.isnull().any()]]\n",
    "### この段階ではdf1は更新されていないことに注意．inplace=Trueとすると上記はエラーとなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 内挿に問題がないことが確認ができたので，inplace=Trueにより，df1を更新する\n",
    "df1.interpolate(method='time', inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内挿によってインデックスの頻度を統一する\n",
    "1990年以前は3時間間隔，1991年以降は1時間間隔となっている．これをすべて内挿によって1時間間隔のDataFrameとする．<br>\n",
    "注意：地点名等，内挿ではなく，ffillで補完すべきものがある．よって，ffillと内挿を分けて実現する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1時間間隔のインデックスを作る\n",
    "new_index = pd.date_range(datetime_ini, datetime_end, freq='1H')\n",
    "#print(new_index)\n",
    "### ffillを適用するカラム名のリスト\n",
    "cols_ffill=['KanID', 'Kname', 'KanID_1', 'lhpaRMK', 'shpaRMK', 'kionRMK', 'stemRMK', 'rhumRMK', 'mukiRMK', 'spedRMK', \\\n",
    "              'clodRMK', 'tnkiRMK', 'humdRMK', 'lghtRMK', 'slhtRMK', 'kousRMK']\n",
    "### カラム全体のリストからffillを適用するカラムを削除するラムダ関数dellistを定義\n",
    "dellist = lambda items, sublist: [item for item in items if item not in sublist]\n",
    "### 内挿を適用するカラム名のリストをつくる\n",
    "cols_interp=dellist(df1.columns, cols_ffill)\n",
    "print(cols_interp)\n",
    "### 1時間間隔のインデックスを適用し，ffillを適用すべきカラムを対象に補完実行．結果をdf_ffillに入れる\n",
    "df_ffill = df1.reindex(new_index).loc[:, cols_ffill].fillna(method='ffill')\n",
    "### カラムKname以外のカラムのdtypeをintに戻す（NaNを扱うとintだったものがfloatに変更されてしまう）\n",
    "df_ffill[dellist(df_ffill.columns, ['Kname'])] = df_ffill[dellist(df_ffill.columns, ['Kname'])].astype(int)\n",
    "### 1時間間隔のインデックスを適用し，時間内挿すべきカラムを対象に内挿実行．結果をdf_interpに入れる\n",
    "df_interp = df1.reindex(new_index).loc[:, cols_interp].interpolate(method='time')\n",
    "### df_ffillとdf_interpを連結し，カラムの順序をdf1と同じとしたデータフレームdfを作る\n",
    "### これで一応完成だが，1990年以前の全天日射量，日照時間，降水量への対応を今後進める\n",
    "df = pd.concat([df_ffill, df_interp], axis=1)[df1.columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成したデータフレームをファイルに出力し，再利用できるようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvファイルに出力し，読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_path = \"../../../met/GWO/src/dat/df_tokyo.csv\"  ### 出力先のディレクトリ\n",
    "df.to_csv(fo_path, encoding='utf-8', )  ### CSVで出力する．デフォルトではWindows版ではShiftJISとなるため，UTF-8を明示する．\n",
    "cmd = 'nkf -w -Lu --overwrite ' + fo_path  ### 改行コードをLinuxタイプのLFに変更しておく（Linuxとの互換性のため）\n",
    "subprocess.call(cmd)\n",
    "df_csv = pd.read_csv(fo_path, index_col=0, parse_dates=True)  ### 出力したCSVをDataFrameとして読み込む\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling mean and create DataFrame\n",
    "インスタンスの時系列範囲に移動平均を施す際に，両端付近は一般にデータが不足するので，移動平均を施す時系列範囲をstartとendで設定し，切り出せるようにした．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if not taking rolling mean, set window=1.\n",
    "window = 1  ### set positive odd number\n",
    "### 時系列の時は日時文字列をparse関数でdatetime.datetimeに変換する\n",
    "start = parse(\"1990-12-2 00:00:00\"); end = parse(\"1991-1-16 00:00:00\")\n",
    "df = df.rolling(window=window, center=True).mean()[start:end]\n",
    "### Resampling\n",
    "# df = df.resample('5D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.datetime_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### date2num()はdatetime.datetimeを数値に変換する関数．\n",
    "### datetime.datetimeを扱えないベクトルプロットquiverで必要になる\n",
    "date2num(met.datetime_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(df.index) == pd.DatetimeIndex:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index[0]:df.index[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classes for extracting 1-D scalar and 1-D vector from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data1Ds:\n",
    "    '''Class for 1-D scalar data\n",
    "       Use subclass Data1D for constructing instance for scalar or vector\n",
    "    '''\n",
    "    def __init__(self, df=None, col_1 = None, xrange: tuple = (0, -1)):\n",
    "        '''df: pandas DataFrame, col_1: selected column of df and its value is v1, \n",
    "           xrange: apply x range of df index or full range of index (0, -1)'''\n",
    "        if xrange == (0, -1):  ### default x full range\n",
    "            self.df = df[df.index[0]:df.index[-1]]\n",
    "        else:  ### x range set via argument\n",
    "            self.df = df[xrange[0]:xrange[1]]\n",
    "        self.xrange = (self.df.index[0], self.df.index[-1])\n",
    "        (self.xmin, self.xmax) = self.xrange\n",
    "        if type(self.df.index) == pd.DatetimeIndex:\n",
    "            self.x = self.df.index.to_pydatetime()  ### pandas datetimeindex => datetime.datetime\n",
    "        else:\n",
    "            self.x = self.df.index\n",
    "        self.v1 = self.df[col_1].values\n",
    "        self.v1max = max(self.v1)\n",
    "        self.v1min = min(self.v1)\n",
    "        self.v1range = (self.v1min, self.v1max)\n",
    "        self.vrange = self.v1range  ### to set y-range automatically by default\n",
    "\n",
    "class Data1D(Data1Ds):\n",
    "    '''Organize 1-D scalar or vector data.  \n",
    "       初期化メソッドの引数にcol_2が存在しない場合はスカラー，存在する場合はベクトルと自動判定\n",
    "    '''\n",
    "    def __init__(self, df=None, col_1 = None, col_2 = None, xrange: tuple = (0, -1)):\n",
    "        ''' USAGE: scalar = Data1D(df, 'kion'), vector = Data1D(df, 'u', 'v')'''\n",
    "        super().__init__(df, col_1, xrange)\n",
    "        if col_2:\n",
    "            self.v2 = self.df[col_2].values\n",
    "            self.v2max = max(self.v2)\n",
    "            self.v2min = min(self.v2)\n",
    "            self.v2range = (self.v2min, self.v2max)\n",
    "            self.v = np.sqrt(self.v1**2 + self.v2**2)  ### v: magnitude of vector\n",
    "            vmax = max(self.v)\n",
    "            self.vrange = (-vmax, vmax)  ### useful for scaling vertical axis of vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data1D(df=df, col_1='kion')\n",
    "data.v1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "if a:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data1D_PlotConfig:\n",
    "    def __init__(self, fig_size: tuple=(10,2), title_size: int=14, label_size: int=12, \\\n",
    "                 plot_color = 'b', xlabel = None, ylabel = None, v_color = 'k', vlabel = None, \\\n",
    "                 vlabel_loc = 'lower right', xlim: tuple = None, ylim: tuple = None, \\\n",
    "                 x_major_locator = None, y_major_locator = None, \\\n",
    "                 x_minor_locator = None, y_minor_locator = None, \\\n",
    "                 grid = False, format_xdata = None, format_ydata = None):\n",
    "        self.fig_size = fig_size\n",
    "        self.title_size = title_size\n",
    "        self.label_size = label_size\n",
    "        self.plot_color = plot_color\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.v_color = v_color  ### color for fill_between of magnitude v\n",
    "        self.vlabel = vlabel\n",
    "        self.vlabel_loc = vlabel_loc\n",
    "        self.xlim = xlim\n",
    "        self.ylim = ylim\n",
    "        self.x_major_locator = x_major_locator\n",
    "        self.y_major_locator = y_major_locator\n",
    "        self.x_minor_locator = x_minor_locator\n",
    "        self.y_minor_locator = y_minor_locator\n",
    "        self.grid = grid\n",
    "        self.format_xdata = format_xdata\n",
    "        self.format_ydata = format_ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot1D:\n",
    "    def __init__(self, plot_config, data):\n",
    "        self.cfg = plot_config\n",
    "        self.data = data\n",
    "        self.figure = plt.figure(figsize=self.cfg.fig_size)\n",
    "\n",
    "    def get_axes(self):\n",
    "        return self.figure.add_subplot(1,1,1)\n",
    "        \n",
    "    def update_axes(self, axes):\n",
    "        ### axes.set_ylim(self.data.v1range[0], self.data.v1range[1])\n",
    "        ### axes.set_xlim(parse(\"2014-01-15\"), parse(\"2014-01-16\"))\n",
    "        if self.cfg.xlim:\n",
    "            axes.set_xlim(self.cfg.xlim)\n",
    "        else:\n",
    "            axes.set_xlim(self.data.x[0], self.data.x[-1])  ### default: full range\n",
    "        if self.cfg.ylim:\n",
    "            axes.set_ylim(self.cfg.ylim)\n",
    "        else:\n",
    "            axes.set_ylim(self.data.vrange[0], self.data.vrange[1])  ### default\n",
    "        axes.grid(self.cfg.grid)\n",
    "        if self.cfg.xlabel:\n",
    "            axes.set_xlabel(self.cfg.xlabel, fontsize = self.cfg.label_size)\n",
    "        if self.cfg.ylabel:\n",
    "            axes.set_ylabel(self.cfg.ylabel, fontsize = self.cfg.label_size)\n",
    "        if self.cfg.format_xdata:\n",
    "            axes.format_xdata = self.cfg.format_xdata  ### DateFormatter('%Y-%m-%d')\n",
    "        if self.cfg.format_ydata:\n",
    "            axes.format_ydata = self.cfg.format_ydata\n",
    "        if self.cfg.x_major_locator:\n",
    "            axes.xaxis.set_major_locator(self.cfg.x_major_locator)\n",
    "        if self.cfg.y_major_locator:\n",
    "            axes.yaxis.set_major_locator(self.cfg.y_major_locator)\n",
    "        if self.cfg.x_minor_locator:\n",
    "            axes.xaxis.set_minor_locator(self.cfg.x_minor_locator)  ### days\n",
    "        if self.cfg.y_minor_locator:\n",
    "            axes.yaxis.set_minor_locator(self.cfg.y_minor_locator)\n",
    "        #fig.autofmt_xdate()\n",
    "        return axes\n",
    "\n",
    "    def make_plot(self, axes):\n",
    "        return axes.plot(self.data.x, self.data.v1, color=self.cfg.plot_color)\n",
    "\n",
    "    def make_quiver(self, axes):\n",
    "        ### Plot vectors and unit vector\n",
    "        #print(type(self.data.x[0]))\n",
    "        if isinstance(self.data.x[0], datetime.datetime):  ### check whether dtype of x is datetime.datetime.\n",
    "            x = date2num(self.data.x)\n",
    "        else:\n",
    "            x = self.data.x\n",
    "        self.q = axes.quiver(x, 0, self.data.v1, self.data.v2, \\\n",
    "                             color=self.cfg.plot_color, units='y', scale_units='y', scale=1, headlength=1, \\\n",
    "                             headaxislength=1, width=0.1, alpha=0.5)\n",
    "        return self.q\n",
    "\n",
    "    def make_quiverkey(self, axes):\n",
    "        return axes.quiverkey(self.q, 0.2, 0.1, 5, '5 m/s', labelpos='W')\n",
    "\n",
    "    def make_fill_between(self, axes):\n",
    "        fill = axes.fill_between(date2num(self.data.x), self.data.v, 0, color= self.cfg.v_color, alpha=0.1)\n",
    "        ### Fake box to insert a legend\n",
    "        p = axes.add_patch(plt.Rectangle((1,1), 1, 1, fc=self.cfg.v_color, alpha=0.1))\n",
    "        leg = axes.legend([p], [self.cfg.vlabel], loc=self.cfg.vlabel_loc)\n",
    "        leg._drawFrame=False\n",
    "\n",
    "    def save_plot(self, filename, **kwargs):\n",
    "        axes = self.update_axes(self.get_axes())\n",
    "        plot = self.make_plot(axes)\n",
    "        self.figure.savefig(filename, **kwargs)\n",
    "\n",
    "    def save_vector_plot(self, filename, magnitude=None, **kwargs):\n",
    "        axes = self.update_axes(self.get_axes())\n",
    "        quiver = self.make_quiver(axes)\n",
    "        quiverkey = self.make_quiverkey(axes)\n",
    "        if magnitude:\n",
    "            fill_between = self.make_fill_between(axes)\n",
    "        self.figure.savefig(filename, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of scalar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xlim = (parse(\"2014-01-15\"), parse(\"2014-02-16\"))  ### ex. for datetime\n",
    "xlim = None\n",
    "ylim = None\n",
    "plot_config = Data1D_PlotConfig(xlim = xlim, ylim = ylim, x_minor_locator = DayLocator(interval=1), y_minor_locator = MultipleLocator(2), \\\n",
    "                                format_xdata = DateFormatter('%Y-%m-%d'), ylabel = 'Temperature (degC)')\n",
    "Plot1D(plot_config, data).save_plot('data.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of vector plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = Data1D(df, 'u', 'v')\n",
    "print(wind.v[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xlim = (parse(\"2013-12-25 00:00:00\"),parse(\"2014-01-10 00:00:00\"))\n",
    "xlim = None \n",
    "ylim = None\n",
    "plot_config = Data1D_PlotConfig(xlim = xlim, ylim = ylim, x_minor_locator = DayLocator(interval=1), \\\n",
    "                                y_minor_locator = MultipleLocator(0.5), format_xdata = DateFormatter('%Y-%m-%d'), \\\n",
    "                                ylabel = 'Wind vector (m/s)', vlabel='Wind speed (m/s)', vlabel_loc = 'lower center')\n",
    "Plot1D(plot_config, wind).save_vector_plot('wind.png', magnitude = True, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
